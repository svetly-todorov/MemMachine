"""OpenAI-based language model implementation."""

import asyncio
import json
import logging
import os
import time
from typing import Any, TypeVar
from urllib.parse import urljoin
from uuid import uuid4

import aiohttp
import openai
from openai.types.responses import Response
from pydantic import BaseModel, Field, InstanceOf

from memmachine.common.data_types import ExternalServiceAPIError
from memmachine.common.metrics_factory import MetricsFactory

from .language_model import LanguageModel

T = TypeVar("T")

logger = logging.getLogger(__name__)

# Environment variable for OpenAI proxy URL
OPENAI_PROXY_URL_ENV = "OPENAI_PROXY_URL"


class OpenAIResponsesLanguageModelParams(BaseModel):
    """
    Parameters for OpenAIResponsesLanguageModel.

    Attributes:
        client (openai.AsyncOpenAI):
            AsyncOpenAI client to use for making API calls.
        model (str):
            Name of the OpenAI model to use
            (e.g. 'gpt-5-nano').
        max_retry_interval_seconds (int):
            Maximal retry interval in seconds when retrying API calls
            (default: 120).
        metrics_factory (MetricsFactory | None):
            An instance of MetricsFactory
            for collecting usage metrics
            (default: None).
        user_metrics_labels (dict[str, str]):
            Labels to attach to the collected metrics
            (default: {}).

    """

    client: InstanceOf[openai.AsyncOpenAI] = Field(
        ...,
        description="AsyncOpenAI client to use for making API calls",
    )
    model: str = Field(
        ...,
        description="Name of the OpenAI model to use (e.g. 'gpt-5-nano')",
    )
    max_retry_interval_seconds: int = Field(
        120,
        description="Maximal retry interval in seconds when retrying API calls",
        gt=0,
    )
    metrics_factory: InstanceOf[MetricsFactory] | None = Field(
        None,
        description="An instance of MetricsFactory for collecting usage metrics",
    )
    user_metrics_labels: dict[str, str] = Field(
        default_factory=dict,
        description="Labels to attach to the collected metrics",
    )


class OpenAIResponsesLanguageModel(LanguageModel):
    """Language model that uses OpenAI's responses API."""

    def __init__(self, params: OpenAIResponsesLanguageModelParams) -> None:
        """
        Initialize the responses language model with configuration.

        Args:
            params (OpenAIResponsesLanguageModelParams):
                Parameters for the OpenAIResponsesLanguageModel.

        """
        super().__init__()

        self._client = params.client

        self._model = params.model

        self._max_retry_interval_seconds = params.max_retry_interval_seconds

        # Check for proxy URL from environment variable
        proxy_url = os.getenv(OPENAI_PROXY_URL_ENV)
        if proxy_url:
            # Ensure the proxy URL ends with /proxy/
            self._proxy_url = urljoin(proxy_url.rstrip("/") + "/", "proxy/v1/responses")
            self._use_proxy = True
            logger.info("Using OpenAI proxy", "proxy_url", self._proxy_url)
        else:
            self._proxy_url = None
            self._use_proxy = False

        metrics_factory = params.metrics_factory

        self._should_collect_metrics = False
        if metrics_factory is not None:
            self._should_collect_metrics = True
            self._user_metrics_labels = params.user_metrics_labels
            label_names = self._user_metrics_labels.keys()

            self._input_tokens_usage_counter = metrics_factory.get_counter(
                "language_model_openai_usage_input_tokens",
                "Number of input tokens used for OpenAI language model",
                label_names=label_names,
            )
            self._input_cached_tokens_usage_counter = metrics_factory.get_counter(
                "language_model_openai_usage_input_cached_tokens",
                (
                    "Number of tokens retrieved from cache "
                    "used for OpenAI language model"
                ),
                label_names=label_names,
            )
            self._output_tokens_usage_counter = metrics_factory.get_counter(
                "language_model_openai_usage_output_tokens",
                "Number of output tokens used for OpenAI language model",
                label_names=label_names,
            )
            self._output_reasoning_tokens_usage_counter = metrics_factory.get_counter(
                "language_model_openai_usage_output_reasoning_tokens",
                ("Number of reasoning tokens used for OpenAI language model"),
                label_names=label_names,
            )
            self._total_tokens_usage_counter = metrics_factory.get_counter(
                "language_model_openai_usage_total_tokens",
                "Number of tokens used for OpenAI language model",
                label_names=label_names,
            )
            self._latency_summary = metrics_factory.get_summary(
                "language_model_openai_latency_seconds",
                "Latency in seconds for OpenAI language model requests",
                label_names=label_names,
            )

    async def generate_parsed_response(
        self,
        output_format: type[T],
        system_prompt: str | None = None,
        user_prompt: str | None = None,
        max_attempts: int = 1,
    ) -> T | None:
        """Generate a structured response parsed into the given model."""
        if max_attempts <= 0:
            raise ValueError("max_attempts must be a positive integer")

        input_prompts = [
            {"role": "system", "content": system_prompt or ""},
            {"role": "user", "content": user_prompt or ""},
        ]

        generate_response_call_uuid = uuid4()

        start_time = time.monotonic()

        try:
            if self._use_proxy:
                response = await self._call_proxy_parse(
                    input_prompts=input_prompts,
                    output_format=output_format,
                    max_attempts=max_attempts,
                )
            else:
                response = await self._client.with_options(
                    max_retries=max_attempts,
                ).responses.parse(
                    model=self._model,  # type: ignore[arg-type]
                    input=input_prompts,  # type: ignore[arg-type]
                    text_format=output_format,
                )
        except openai.OpenAIError as e:
            error_message = (
                f"[call uuid: {generate_response_call_uuid}] "
                "Giving up generating response "
                f"due to non-retryable {type(e).__name__}"
            )
            logger.exception(error_message)
            raise ExternalServiceAPIError(error_message) from e
        except Exception as e:
            error_message = (
                f"[call uuid: {generate_response_call_uuid}] "
                "Giving up generating response "
                f"due to error: {type(e).__name__}"
            )
            logger.exception(error_message)
            raise ExternalServiceAPIError(error_message) from e

        end_time = time.monotonic()

        self._collect_metrics(
            response,
            start_time,
            end_time,
        )

        return response.output_parsed

    async def generate_response(
        self,
        system_prompt: str | None = None,
        user_prompt: str | None = None,
        tools: list[dict[str, Any]] | None = None,
        tool_choice: str | dict[str, str] | None = None,
        max_attempts: int = 1,
    ) -> tuple[str, Any]:
        """Generate a raw text response (and optional tool call)."""
        if max_attempts <= 0:
            raise ValueError("max_attempts must be a positive integer")

        input_prompts = [
            {"role": "system", "content": system_prompt or ""},
            {"role": "user", "content": user_prompt or ""},
        ]

        generate_response_call_uuid = uuid4()

        start_time = time.monotonic()

        sleep_seconds = 1
        for attempt in range(1, max_attempts + 1):
            try:
                logger.debug(
                    "[call uuid: %s] "
                    "Attempting to generate response using %s OpenAI language model: "
                    "on attempt %d with max attempts %d",
                    generate_response_call_uuid,
                    self._model,
                    attempt,
                    max_attempts,
                )
                if self._use_proxy:
                    response = await self._call_proxy_create(
                        input_prompts=input_prompts,
                        tools=tools,
                        tool_choice=tool_choice if tool_choice is not None else "auto",
                    )
                else:
                    response = await self._client.responses.create(
                        model=self._model,
                        input=input_prompts,
                        tools=tools,
                        tool_choice=tool_choice if tool_choice is not None else "auto",
                    )  # type: ignore
                break
            except (
                openai.RateLimitError,
                openai.APITimeoutError,
                openai.APIConnectionError,
            ) as e:
                # Exception may be retried.
                if attempt >= max_attempts:
                    error_message = (
                        f"[call uuid: {generate_response_call_uuid}] "
                        "Giving up generating response "
                        f"after failed attempt {attempt} "
                        f"due to retryable {type(e).__name__}: "
                        f"max attempts {max_attempts} reached"
                    )
                    logger.exception(error_message)
                    raise ExternalServiceAPIError(error_message) from e

                logger.info(
                    "[call uuid: %s] "
                    "Retrying generating response in %d seconds "
                    "after failed attempt %d due to retryable %s...",
                    generate_response_call_uuid,
                    sleep_seconds,
                    attempt,
                    type(e).__name__,
                )
                await asyncio.sleep(sleep_seconds)
                sleep_seconds *= 2
                sleep_seconds = min(sleep_seconds, self._max_retry_interval_seconds)
                continue
            except openai.OpenAIError as e:
                error_message = (
                    f"[call uuid: {generate_response_call_uuid}] "
                    "Giving up generating response "
                    f"after failed attempt {attempt} "
                    f"due to non-retryable {type(e).__name__}"
                )
                logger.exception(error_message)
                raise ExternalServiceAPIError(error_message) from e

        end_time = time.monotonic()
        logger.debug(
            "[call uuid: %s] Response generated in %.3f seconds",
            generate_response_call_uuid,
            end_time - start_time,
        )

        self._collect_metrics(
            response,
            start_time,
            end_time,
        )

        if response.output is None:
            return (response.output_text or "", [])

        try:
            function_calls_arguments = [
                {
                    "call_id": output.call_id,
                    "function": {
                        "name": output.name,
                        "arguments": json.loads(output.arguments),
                    },
                }
                for output in response.output
                if output.type == "function_call"
            ]
        except json.JSONDecodeError as e:
            raise ValueError("JSON decode error") from e

        return (
            response.output_text,
            function_calls_arguments,
        )

    def _collect_metrics(
        self,
        response: Response,
        start_time: float,
        end_time: float,
    ) -> None:
        if self._should_collect_metrics:
            if response.usage is not None:
                self._input_tokens_usage_counter.increment(
                    value=response.usage.input_tokens,
                    labels=self._user_metrics_labels,
                )
                self._input_cached_tokens_usage_counter.increment(
                    value=response.usage.input_tokens_details.cached_tokens,
                    labels=self._user_metrics_labels,
                )
                self._output_tokens_usage_counter.increment(
                    value=response.usage.output_tokens,
                    labels=self._user_metrics_labels,
                )
                self._output_reasoning_tokens_usage_counter.increment(
                    value=response.usage.output_tokens_details.reasoning_tokens,
                    labels=self._user_metrics_labels,
                )
                self._total_tokens_usage_counter.increment(
                    value=response.usage.total_tokens,
                    labels=self._user_metrics_labels,
                )

            self._latency_summary.observe(
                value=end_time - start_time,
                labels=self._user_metrics_labels,
            )

    async def _call_proxy_parse(
        self,
        input_prompts: list[dict[str, str]],
        output_format: type[T],
        max_attempts: int,
    ) -> Response:
        """Call the proxy server for responses.parse."""
        if not self._proxy_url:
            raise ValueError("Proxy URL not configured")

        # Convert Pydantic model class to JSON schema for text_format
        # The OpenAI API expects text_format as a JSON schema
        try:
            if issubclass(output_format, BaseModel):
                text_format_schema = output_format.model_json_schema()
            else:
                # Fallback: try to get schema from the type
                text_format_schema = {"type": "string"}
        except TypeError:
            # output_format is not a class, use string fallback
            text_format_schema = {"type": "string"}

        # Construct the request body as OpenAI expects
        request_body = {
            "model": self._model,
            "input": input_prompts,
            "text_format": text_format_schema,
        }

        timeout = aiohttp.ClientTimeout(total=300.0)
        for attempt in range(1, max_attempts + 1):
            try:
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.post(
                        self._proxy_url,
                        json=request_body,
                        headers={"Content-Type": "application/json"},
                    ) as response:
                        response.raise_for_status()

                        # Parse the response as OpenAI Response type
                        response_data = await response.json()
                        return Response(**response_data)
            except aiohttp.ClientResponseError as e:
                if attempt >= max_attempts:
                    # Convert HTTP errors to OpenAI-like errors for consistency
                    if e.status == 429:
                        raise openai.RateLimitError(
                            f"Rate limit error: {e.message}",
                            response=None,
                            body=e.message,
                        ) from e
                    elif e.status >= 500:
                        raise openai.APIConnectionError(
                            f"Server error: {e.message}",
                            request=None,
                        ) from e
                    else:
                        raise openai.APIError(
                            f"API error: {e.message}",
                            request=None,
                            response=None,
                        ) from e
                # Retry on server errors
                if e.status >= 500:
                    await asyncio.sleep(2 ** (attempt - 1))
                    continue
                raise
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                if attempt >= max_attempts:
                    raise openai.APIConnectionError(
                        f"Connection error: {str(e)}",
                        request=None,
                    ) from e
                await asyncio.sleep(2 ** (attempt - 1))
                continue

        raise ExternalServiceAPIError("Failed to call proxy after all attempts")

    async def _call_proxy_create(
        self,
        input_prompts: list[dict[str, str]],
        tools: list[dict[str, Any]] | None = None,
        tool_choice: str | dict[str, str] = "auto",
    ) -> Response:
        """Call the proxy server for responses.create."""
        if not self._proxy_url:
            raise ValueError("Proxy URL not configured")

        # Construct the request body as OpenAI expects
        request_body: dict[str, Any] = {
            "model": self._model,
            "input": input_prompts,
            "tool_choice": tool_choice,
        }
        if tools is not None:
            request_body["tools"] = tools

        timeout = aiohttp.ClientTimeout(total=300.0)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(
                self._proxy_url,
                json=request_body,
                headers={"Content-Type": "application/json"},
            ) as response:
                response.raise_for_status()

                # Parse the response as OpenAI Response type
                response_data = await response.json()
                return Response(**response_data)
